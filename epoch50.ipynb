{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import natsort\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import model_selection\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"data/train.zip\"\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1      9\n",
       "1   2      0\n",
       "2   3      0\n",
       "3   4      3\n",
       "4   5      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:20<00:00, 2882.18it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'train'\n",
    "\n",
    "IMG_SIZE=100\n",
    "data=[]\n",
    "labels_240=[]\n",
    "i = 0\n",
    "for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "    path = os.path.join(TRAIN_DIR,img)\n",
    "    img_data = cv2.imread(path)\n",
    "    img_data = cv2.resize(img_data, (IMG_SIZE,IMG_SIZE))\n",
    "    image=img_to_array(img_data)\n",
    "    data.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:59<00:00, 1007.00it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_240=[]\n",
    "for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "    number = int(img[:-4])  \n",
    "    categ_num = int(df[df.id==number].label)\n",
    "    labels_240.append(categ_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(labels_240))\n",
    "data=np.array(data,dtype=\"float32\")/255.0\n",
    "labels_240=np.array(labels_240)\n",
    "lb=LabelBinarizer()\n",
    "labels_240=lb.fit_transform(labels_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range = 0.1,\n",
    "                            height_shift_range = 0.1,\n",
    "                            width_shift_range = 0.1,\n",
    "                            rotation_range = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),padding=\"same\",activation=\"linear\",input_shape=(100,100,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding=\"same\",activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Conv2D(64,(3,3),padding=\"same\",activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding=\"same\",activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128,(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10\n",
    "                ,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=model_selection.train_test_split(data,labels_240,test_size=0.25,random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  len(x_train)\n",
    "  len(y_train)\n",
    "  len(y_test)\n",
    "  len(x_test)\n",
    "  print('')\n",
    "  len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 33, 33, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 33, 33, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              6423552   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 6,726,186\n",
      "Trainable params: 6,723,498\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug=ImageDataGenerator(rotation_range=25,width_shift_range=0.1,height_shift_range=0.1,shear_range=0.2,horizontal_flip=True,fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint(filepath='data/model_best2.hdf5',\n",
    "                           save_best_only=True,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "300/300 [==============================] - 479s 2s/step - loss: 1.4556 - acc: 0.5614 - val_loss: 1.6217 - val_acc: 0.4680\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62171, saving model to data/model_best2.hdf5\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 468s 2s/step - loss: 1.0039 - acc: 0.6714 - val_loss: 1.0119 - val_acc: 0.6532\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62171 to 1.01193, saving model to data/model_best2.hdf5\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 450s 2s/step - loss: 0.8847 - acc: 0.7016 - val_loss: 1.1887 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01193\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.8000 - acc: 0.7199 - val_loss: 0.9032 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01193 to 0.90320, saving model to data/model_best2.hdf5\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 423s 1s/step - loss: 0.7531 - acc: 0.7347 - val_loss: 0.8688 - val_acc: 0.6574\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.90320 to 0.86879, saving model to data/model_best2.hdf5\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 429s 1s/step - loss: 0.7145 - acc: 0.7432 - val_loss: 0.7719 - val_acc: 0.7021\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.86879 to 0.77189, saving model to data/model_best2.hdf5\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 423s 1s/step - loss: 0.6604 - acc: 0.7599 - val_loss: 0.6125 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.77189 to 0.61246, saving model to data/model_best2.hdf5\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 420s 1s/step - loss: 0.6352 - acc: 0.7717 - val_loss: 0.5995 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61246 to 0.59954, saving model to data/model_best2.hdf5\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 424s 1s/step - loss: 0.5922 - acc: 0.7832 - val_loss: 0.5200 - val_acc: 0.8058\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.59954 to 0.51996, saving model to data/model_best2.hdf5\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.5824 - acc: 0.7869 - val_loss: 0.6026 - val_acc: 0.7885\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51996\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 424s 1s/step - loss: 0.5607 - acc: 0.7927 - val_loss: 0.6844 - val_acc: 0.7441\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51996\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 423s 1s/step - loss: 0.5465 - acc: 0.8030 - val_loss: 0.5401 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51996\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 424s 1s/step - loss: 0.5287 - acc: 0.8068 - val_loss: 0.4451 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.51996 to 0.44514, saving model to data/model_best2.hdf5\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.5073 - acc: 0.8141 - val_loss: 0.7334 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44514\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 428s 1s/step - loss: 0.5038 - acc: 0.8171 - val_loss: 0.4558 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44514\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 424s 1s/step - loss: 0.4891 - acc: 0.8227 - val_loss: 0.4450 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.44514 to 0.44497, saving model to data/model_best2.hdf5\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.4787 - acc: 0.8245 - val_loss: 0.4961 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44497\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 430s 1s/step - loss: 0.4593 - acc: 0.8351 - val_loss: 0.4334 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.44497 to 0.43336, saving model to data/model_best2.hdf5\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 431s 1s/step - loss: 0.4680 - acc: 0.8285 - val_loss: 0.5057 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43336\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 426s 1s/step - loss: 0.4341 - acc: 0.8410 - val_loss: 0.3474 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.43336 to 0.34742, saving model to data/model_best2.hdf5\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.4441 - acc: 0.8377 - val_loss: 0.3998 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34742\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.4305 - acc: 0.8405 - val_loss: 0.3737 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34742\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 428s 1s/step - loss: 0.4285 - acc: 0.8443 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.34742\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 431s 1s/step - loss: 0.4285 - acc: 0.8431 - val_loss: 0.3643 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34742\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 454s 2s/step - loss: 0.4139 - acc: 0.8477 - val_loss: 0.3481 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.34742\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 435s 1s/step - loss: 0.4110 - acc: 0.8491 - val_loss: 0.3820 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.34742\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 429s 1s/step - loss: 0.4071 - acc: 0.8509 - val_loss: 0.4341 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.34742\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3958 - acc: 0.8548 - val_loss: 0.3301 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.34742 to 0.33014, saving model to data/model_best2.hdf5\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3854 - acc: 0.8581 - val_loss: 0.3370 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.33014\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 431s 1s/step - loss: 0.3809 - acc: 0.8594 - val_loss: 0.3278 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.33014 to 0.32783, saving model to data/model_best2.hdf5\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.3800 - acc: 0.8589 - val_loss: 0.5068 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.32783\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3781 - acc: 0.8594 - val_loss: 0.3784 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.32783\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 428s 1s/step - loss: 0.3758 - acc: 0.8610 - val_loss: 0.3206 - val_acc: 0.8823\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.32783 to 0.32062, saving model to data/model_best2.hdf5\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 438s 1s/step - loss: 0.3804 - acc: 0.8608 - val_loss: 0.3065 - val_acc: 0.8869\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.32062 to 0.30654, saving model to data/model_best2.hdf5\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 461s 2s/step - loss: 0.3696 - acc: 0.8632 - val_loss: 0.3910 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.30654\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 452s 2s/step - loss: 0.3653 - acc: 0.8655 - val_loss: 0.4682 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.30654\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 431s 1s/step - loss: 0.3570 - acc: 0.8681 - val_loss: 0.2911 - val_acc: 0.8959\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.30654 to 0.29111, saving model to data/model_best2.hdf5\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 425s 1s/step - loss: 0.3486 - acc: 0.8717 - val_loss: 0.3545 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.29111\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 426s 1s/step - loss: 0.3610 - acc: 0.8689 - val_loss: 0.3620 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.29111\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 428s 1s/step - loss: 0.3488 - acc: 0.8717 - val_loss: 0.3024 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.29111\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 432s 1s/step - loss: 0.3539 - acc: 0.8684 - val_loss: 0.2950 - val_acc: 0.8939\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.29111\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 429s 1s/step - loss: 0.3400 - acc: 0.8756 - val_loss: 0.3532 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.29111\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3402 - acc: 0.8736 - val_loss: 0.3791 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.29111\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3365 - acc: 0.8764 - val_loss: 0.3588 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.29111\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3294 - acc: 0.8795 - val_loss: 0.2984 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.29111\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 430s 1s/step - loss: 0.3381 - acc: 0.8759 - val_loss: 0.3230 - val_acc: 0.8851\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.29111\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 448s 1s/step - loss: 0.3285 - acc: 0.8787 - val_loss: 0.3163 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.29111\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 457s 2s/step - loss: 0.3250 - acc: 0.8796 - val_loss: 0.2856 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.29111 to 0.28564, saving model to data/model_best2.hdf5\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 433s 1s/step - loss: 0.3266 - acc: 0.8813 - val_loss: 0.2696 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.28564 to 0.26957, saving model to data/model_best2.hdf5\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 427s 1s/step - loss: 0.3205 - acc: 0.8833 - val_loss: 0.2916 - val_acc: 0.8961\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.26957\n"
     ]
    }
   ],
   "source": [
    "train=model.fit_generator(aug.flow(x_train,y_train,batch_size=64),validation_data=(x_test,y_test),\n",
    "                          steps_per_epoch=len(x_train)/150,epochs=50,verbose=1,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
